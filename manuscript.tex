\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{color}

\newcommand{\TODO}[1]{\textcolor{red}{[TODO: #1]}}

\title{Think again?\\ Optimal mental simulation tracks problem difficulty}

\author{{\large \bf Jessica B.~Hamrick (jhamrick@berkeley.edu)} \\
  Department of Psychology \\
  University of California, Berkeley
  \AND {\large \bf Kevin A.~Smith (k2smith@ucsd.edu)} \\
  Department of Psychology \\
  University of California, San Diego
  \AND {\large \bf Thomas L.~Griffiths (tom\_griffiths@berkeley.edu)} \\
  Department of Psychology \\
  University of California, Berkeley
  \AND {\large \bf Edward Vul (evul@ucsd.edu)} \\
  Department of Psychology \\
  University of California, San Diego}

\begin{document}

\maketitle

\begin{abstract}
\TODO{}

\textbf{Keywords:} 
add your choice of indexing terms or keywords; kindly use a
semicolon; between each term
\end{abstract}

\section{Introduction}

%% The question we're interested in: how many simulations do people take?
Imagine you are playing a video game---for example, Angry Birds or Desert Golf---in which you need to predict where an object will end up. Recent research in the domain of intuitive physics and mental simulation has suggested that people make predictions like these by running noisy physical simulations \cite{Smith2012,Battaglia2013,Smith:2013ug,Smith:2013th,Smith:2014tx,Ullman:2014ut,Hamrick:2015}. However, while this research has investigated the \emph{mechanism} by which people make these types of predictions, there has been very little investigation into \emph{how} people actually use this mechanism. For example, if people are running mental simulations, \emph{how many} simulations do they run?

\TODO{something discussing that there are scenarios in which it's useful to take more simulations than others?}

%% What's already been done
Previous research by \citeA{Vul:2014ba} has asked, in the abstract, how many samples an agent should take before making a decision. Surprisingly, they found that to optimize success in the long run, it is not useful for sample-based agents to take more than a few samples. For example, suppose that there is a reward for correct answers and no penalty for incorrect answers, and that the agent can take ten samples in the time it takes to actually execute the action that reflects the decision. Then, the optimal number of samples that the agent should base their decision on is only one! While this is obviously suboptimal for each individual decision, over many decisions, this strategy trades off between the amount of time one can actually spend taking actions versus thinking.

%% What's missing
So far, the theoretical results from \citeA{Vul:2014ba} have largely not been applied to the domain of mental simulation, and lead to several questions regarding how people use their mental simulations. First, do people run multiple mental simulations? If so, do they run a fixed number of simulations, or vary the number of simulations based on the difficult? A supplementary analysis by \citeA{Battaglia2013} suggests that people do run multiple simulation. In their analysis, they attempted to determine the number of samples taken by people in several tasks concerning towers of building blocks. They did find a consistent story with \citeA{Vul:2014ba}: participants in their experiments seemed to use between three and seven samples per judgment. However, their analysis only concerned the variability of responses and did not take into account response time.

%% What we're going to do and how we're going to do it
In this paper, we investigate how many mental simulations people run when making decisions in a two alternative force choice task, analyzing both their response times and the variability of responses. \TODO{plan of the paper}

\section{Modeling mental simulation with SPRT}

% SPRT
\citeA{Vul:2014ba} examined not just the case where an agent can take a fixed number of samples, but also several policies for taking a variable number of samples depending on how useful the samples are. For example, consider the case where an agent must decide between two options. If they draw samples favoring one of the options with probability 0.95, then the agent can be fairly confident about which is the better option after only a few samples. In contrast, if the probability is 0.55, then the agent would need many samples to be confident about which option is better. Of course, the agent doesn't know what the probability is: that is the point of sampling. One strategy which addresses this issue is known as the \emph{sequential probability ratio test} \cite{wald1947sequential}, or SPRT, and has been used extensively to model a variety of decision making tasks in humans \cite<e.g.>{Gold:2007fo,Ratcliff:2008ux,Bitzer:2014ea}. An agent using SPRT accumulates evidence for one option versus another, and stops sampling once a particular threshold of evidence has been reached. So, rather than optimizing for a fixed number of samples, the agent should optimize for the threshold.

\TODO{probably don't actually need to go through all the math here, can just cite the appendix of one and done?}

% briefly go over one and done math
Here, we consider the binary (or two-alternative forced choice) case, where an agent must choose one of two hypotheses, $H_0$ or $H_1$. The agent may take samples $X_i$ from a Bernoulli distribution parameterized by an unknown parameter $p$, and from these samples estimate $\hat{p}=\frac{1}{N}\sum_{i=1}^N X_i$, where $N$ is the total number of samples. Then, the decision rule which minimizes the probability of error is $\hat{H}(X_1,\ldots{},X_N)=H_0$ when $\hat{p}<0.5$ and $\hat{H}(X_1,\ldots{},X_N)=H_1$ when $\hat{p}>0.5$.

In the best possible case, the agent takes infinite samples and chooses the maximum \emph{a posteriori} (MAP) hypothesis with probability $p$. In practice, the agent cannot take infinite samples. Thus, to determine when to stop sampling (i.e., what the value of $N$ is), we use the \emph{sequential probability ratio test} \cite{wald1947sequential}. When using a SPRT strategy, the agent accumulates $Y_N=\sum_{i=1}^N 2X_i-1$, and stops either when $Y_N=T$ or $Y_N=-T$, for some threshold number of samples $T$. Independent of the number of samples actually taken, the probability of choosing $H_1$ given that $H_1$ is the MAP hypothesis is:
\begin{equation}
\Pr[\hat{H}(Y_N)=H_1\,|\,H_1]=\frac{p^T}{p^T+(1-p)^T},
\label{eq:pr-choose-h1}
\end{equation}
and so the marginal probability of correctly choosing the MAP hypothesis is:
\begin{equation}
\Pr[\mathrm{correct}]=\frac{p^{T+1}+(1-p)^{T+1}}{p^T+(1-p)^T}.
\label{eq:pr-correct}
\end{equation}
Then, the expected utility for a decision, given a threshold and the true probability, is:
\begin{equation}
\mathbb{E}[U\,|\,T,p]=\Pr[\mathrm{correct}]\cdot{}u^{(+)}+(1-\Pr[\mathrm{correct}])\cdot{}u^{(-)},
\label{eq:expected-utility}
\end{equation}
where $u^{(+)}$ is the utility for a correct answer and $u^{(-)}$ is the utility for an incorrect answer.

We are interested not just in how much utility is gained from a single decision, but how much utility is gained from \emph{each additional sample}. Under the SPRT strategy, the probability of taking $N$ samples and then choosing $H_1$ is:
\begin{equation}
\Pr[N,H_1\,|\,T,p]=\left(\frac{2^N}{2T}\right)p^{\frac{N+T}{2}}(1-p)^{\frac{N-T}{2}}\sum_{\nu=1}^{2T-1}\cos\left(\frac{\nu\pi}{2T}\right)^{N-1}\sin\left(\frac{\nu\pi}{2T}\right)\sin\left(\frac{\nu\pi}{2}\right)
\end{equation}
for $N\geq T$ and where $N$ is of the same parity as $T$ \cite[ch.~XIV, eq. 5.7]{Feller:1968ut}. Because we are dealing with binary hypotheses, the marginal probability of $N$ samples is then:
\begin{equation}
\Pr[N\,|\,T,p]=\Pr[N,H_1\,|\,T,p]+\Pr[N,H_1\,|\,T,1-p].
\label{eq:pr-n}
\end{equation}

Putting together Equations \ref{eq:expected-utility} and \ref{eq:pr-n}, the expected rate or return, or utility over time, is:
\begin{equation}
\mathbb{E}[U/t\,|\,T,t_a,t_s]=\int_0^1 \sum_{N=T}^\infty \frac{\mathbb{E}[U\,|\,T,p]\cdot{}t_a}{Nt_s+t_a}\Pr[N\,|\,T,p]\Pr[p]\ \mathrm{d}p,
\label{eq:ror-fixed}
\end{equation}
where $t_s$ is the time it takes to draw a single sample, and $t_a$ is the time it takes to make a decision or execute an action \cite{Vul:2014ba}. Finally, given this equation, we can compute the optimal threshold for an agent using the SPRT strategy:
\begin{equation}
T^*=\mathrm{arg}\max_T \mathbb{E}[U/t\,|\,T,t_a,t_s].
\label{eq:optimal-threshold}
\end{equation}

\section{Modeling physical predictions}

% describe model from sources of uncertainty
As discussed in the introduction, recent research suggests that people can reason about physical scenarios by running noisy physics simulations \cite<e.g.>{Smith2012,Battaglia2013,Smith:2013ug,Smith:2013th,Smith:2014tx,Ullman:2014ut,Hamrick:2015}. Here, we assume this to be the case, and use the model from \citeA{Smith2012}. Briefly, their model incorporates four sources of uncertainty into the physical simulations: perceptual uncertainty over the position of the ball ($\sigma_p$), uncertainty in the initial direction of velocity ($\kappa_v$), ongoing uncertainty in the direction of movement ($\kappa_m$), and uncertainty in the bounce angle ($\kappa_b$). We fit these model parameters to human judgments in a related but separate task. Then, to determine the probability that the ball goes in the hole, we simulated $M=10000$ samples from the model. From these samples, we estimated a truncated normal distribution along the wall with the hole in it, and then computed the probability density lying within the region of the hole.

\TODO{talk about standard deviation adjustment}

\TODO{talk about adjusting for the number of bounces -- or should this be in the results?}

\section{Experiment 1: Replicating Smith and Vul (2014)}

Experiment 1 was a replication of the ``pong'' experiment from \cite{Smith2012}, with the major difference being a new set of stimuli and that it was run on Amazon's Mechanical Turk rather than in the lab.

\subsection{Participants}

We recruited $N=60$ participants on Amazon's Mechanical Turk using the psiTurk \cite{McDonnell12} experimental framework. Participants were treated in accordance with UC Berkeley IRB standards and were paid \$0.60 for approximately 5 minutes of work. Additionally, we excluded $N=18$ participants from analysis for failing to catch the ball on more than one control trial, leaving a total of $N=42$ participants.

\subsection{Stimuli}

There were 48 unique stimuli, each consisting of an animation depicting a blue ball with a radius of 10px bouncing around in a box with dimensions 900px $\times$ 650px, as well as a ``paddle'' that was fixed at some $x$-coordinate but moveable along the $y$-axis. Each animation had a duration of 0.775 seconds and depicted the ball moving in a particular direction with a velocity of 400px/s. Additionally, the path that the ball had traveled so far was drawn with a faded gray line (see Figure \ref{fig:experiment}).

Each animation was chosen such that were the ball to continue moving after the end of the animation, it would take exactly \TODO{} seconds to reach the $y$-coordinate of the paddle.\footnote{To enforce the constraint that the ball always travel the same distance during the feedback animation, the $x$-coordinate of the wall with the hole in it was allowed to vary across stimuli.} During this ``gap'' period in between the end of the animation and the ball reaching the $x$-coordinate of the paddle, the ball could bounce on the top, right, or bottom walls either 0, 1, or 2 times.

In addition to the 48 experimental trials, there were seven instruction trials and eight control trials. The control trials were designed to be extremely easy and consisted of a ball heading straight towards the $x$-coordinate of the paddle (with a angle of incidence of either $90^\circ$ or close to $90^\circ$. Thus, participants saw a total of 63 trials throughout the experiment.

\subsection{Procedure}

The experiment was divided into two phases: the training phase, and the experimental phase. During the training phase, participants made judgments on the seven instruction trials in order to get used to the task. During the experimental phase, participants made judgments on the 48 experimental trials, presented in a random order, as well as the eight control trials, which were also shown in a random order, but interspersed with the experimental trials such that every 8th trial was a control trial.

On each trial, participants were shown the scene, including the initial position of the ball and the location of the hole. The paddle begin at the center of the $y$-axis, and was freely moveable along the $y$-axis from the very beginning of the trial. Participants were instructed to press the ``space'' key to begin the trial. Immediately upon pressing ``space'', the initial stimulus presentation began. As soon as the initial stimulus animation concluded, a gray box was drawn over the screen, occluding the ball (but not the line depicting the path it had traveled so far; this was left in as a reminder to participants of where the ball had come from). As soon as the occluder appeared, a timer also appeared and begin counting down for \TODO{} seconds. During this time, participants had to move the paddle to try to catch the ball in the position it would be when the timer was up.

When the timer countdown finished, the paddle froze at its current location, and the occluder was removed and the full path of the ball revealed. If the ball was on the paddle, then participants were told, ``You caught the ball!''. If the ball was not on the paddle, participants were told ``Oops, you didn't catch the ball.'' Participants were then instructed to press ``space'' to continue, at which point the next trial would begin.

Throughout the experiment, participants saw two counters: one showing their total progress (the number of trials completed / total trials), and their score (the number of times they caught the ball / the number of trials completed).

\subsection{Results}

\section{Experiment 2}

\subsection{Participants}

We recruited $N=328$ participants on Amazon's Mechanical Turk using the psiTurk \cite{McDonnell12} experimental framework. Participants were treated in accordance with UC Berkeley IRB standards and were paid \$0.60 for approximately 6.5 minutes of work. Participants were randomly assigned to one of eight conditions, which determined which stimuli they judged based on a latin-square design (see Stimuli). Additionally, we excluded $N=8$ participants from analysis for answering incorrectly on more than one control trial (see Stimuli), leaving a total of $N=320$ participants.

\subsection{Stimuli}

The stimuli were modified versions of those used in Experiment 1, with two main differences. First, instead of a moveable paddle, there was a wall with a fixed hole in it; the wall was at the same $x$-coordinate as the paddle. Second, while the initial stimulus presentation was identical, the stimuli in Experiment 2 had a second animation which was shown after participants responded. The feedback, which picked up immediately where the first animation left off, had a duration of 1.5 seconds. It depicted the ball either going into the hole or bouncing off the wall that contained the hole.

There were 48 different initial animations (corresponding to the animations used in Experiment 1), and for each of these intial animations, there were four different types of feedback and two different hole sizes, giving a total of eight versions of each stimulus. The four feedback types were: ``far in'' (FI), where the ball went directly through the center of the hole; ``far miss'' (FM), where the ball missed the hole by a wide margin; ``close in'' (CI), where the ball just barely went through the hole; and `` close miss'' (CM), where the ball just barely missed the hole. The two hole sizes were 100px and 200px.

In order to ensure that participants never saw the same initial animation twice, we used a latin-square design of Initial Animation $\times$ Hole Type $\times$ Hole Size. Thus, each participant saw each initial animation exactly once, each hole type exactly 12 times, and each hole size exactly 24 times.

The training trials and control trials were also the same as those used in Experiment 1, with feedback animations that were the same across participants. The feedback and hole sizes of the control trials were designed to be extremely easy and were either of type ``straight hit'' (with a hole size of either 300px or 350px) or ``far miss'' (with a hole size of 100px).

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/experiment.png}
        \caption{\textbf{Example experimental trial.} Each panel shows a different part of the trial. The left panel shows the initial screen presented to the participant. The middle panel shows the occluded ball, after observing the stimulus presentation. The faded gray line shows the path the ball took during the initial presentation. The right panel shows the final position of the ball, after observing the feedback. As in the middle panel, the faded gray line shows the path of the ball.}
        \label{fig:experiment}
    \end{center}
\end{figure*}

\subsection{Procedure}

The procedure was very similar to that in Experiment 1, and was identical up until the point that the occluder was shown. Instead of then having a fixed amount of time to answer, participants had unlimited time. Participants were asked, ``will the ball go in the hole?'', and were instructed to press `q' to respond in the affirmative, and `p' to respond in the negative. Immediately after responding, text appeared saying ``Correct!'' or ``Incorrect.'', depending on the participants' response. Additionally, the gray occluder was removed, and participants were shown the feedback animation. After the feedback animation was complete, the final frame of the animation remained on the screen until participants pressed ``space'' to advance to the next trial. Figure \ref{fig:experiment} shows a few frames from one of the experimental trials.

\section{Results}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/human-pct-vs-rt.pdf}
        \caption{\textbf{Response times as a function of response.} Left subplot: each bar shows the proportion of participants saying that the ball will go in the hole for a particular feedback type ($x$-axis) and hole size (color). Middle subplot: like the left subplot, but the $y$-axis shows bootstrapped logarithmic means of response times. Right subplot: each point corresponds to a different stimulus, feedback type, and hole size.  The $x$-axis is the proportion of participants saying the ball will go in the hole, and the $y$ axis is the logarithmic mean response time.}
        \label{fig:pct-vs-rt}
    \end{center}
\end{figure*}

\subsection{Responses}

On average, participants were correct 76.1\% of the time ($N=17686$) and responded that the ball would go in the hole 52.8\% of the time ($N=17686$). There was a significant effect of feedback type on participants' responses ($\chi^2(3)=4477.2, p<0.001$) as well as a significant effect of hole size ($\chi^2(1)=168.6, p<0.001$). Additionally, there was an interaction between feedback type and hole size ($\chi^2(3)=64.469, p<0.001$). Figure \ref{fig:pct-vs-rt} (left subplot) shows responses as a function of feedback type and hole size.

\subsection{Response times}

For all analyses of response time, we computed averages using bootstrapped logarithmic means (exponential of the mean of the log response times), using 1000 bootstrap samples. Participants had an average response time of $M=1310.9$ milliseconds ($SD=1296.6$ms). There were significant effects of both feedback type ($\chi^2(3)=63.611, p<0.001$) and hole size ($\chi^2(1)=8.9808, p<0.01$) on response time, as well as an interaction between feedback type and hole size ($\chi^2(3)=27.146, p<0.001$). However, the hole size was only significant in the case of the ``far in'' (FI) trials ($p<0.001$), in which the ball went straight through the center of the hole. Figure \ref{fig:pct-vs-rt} (middle subplot) shows the response times as a function of feedback type and hole size.

\subsection{Relationship of responses to reaction time}

As shown in Figure \ref{fig:pct-vs-rt} (right subplot), we found a clear speed-accuracy tradeoff in participants responses and response times. In particular, participants were on average faster to respond on trials that they were more certain about, and slower to respond to trials that they were unsure about. This tradeoff mirrors the same tradeoff found in SPRT: decisions for which $p\approx0.5$ take much longer to make, because it takes longer to get to one threshold versus the other.

\subsection{Comparison to SPRT}

We fit participant responses and response times to the SPRT model with different values of $T$, and found that $T=2$ to have the best fit. Participant responses were well correlated with the SPRT model ($r=0.81$, 95\% CI [0.77, 0.84]), and slightly less so with a SPRT model with $T=2$ ($r=0.78$, 95\% CI [0.74, 0.81]). There was also a significant correlation with resonse time for the $T=2$ SPRT model ($r=0.35$, 95\% CI [0.27, 0.43]). However, it turns out that the number of bounces is a much better predictor of people's response times than the model itself. At first glance, this seems like it could contradict the earlier result that unknown sampling times have little effect on the optimal number of samples to take. An alternate possibility is just that each bounce adds time to people's simulations above and beyond what it adds to the model simulations, but does not actually affect the number of samples they take. More investigation will be required to fully understand what is going on here.

\section{Discussion}

% In this paper, we asked the question: how should a sample-based agent act when it does not know how long it takes to draw a sample? To answer this question, we examined the optimal behavior of agents using the sequential probability ratio test (SPRT) with unknown time costs. Surprisingly, we found that the uncertainty introduced by the unknown sample times did not have a large effect: in most cases, the optimal threshold is the same regardless.

% We additionally ran an experiment to see how well SPRT predicts human behavior on a task which enforces unknown time costs (as well as a range of underlying probabilities). While we did find a clear relationship between people's responses and their reaction time, analagous to the type of relationship that would be expected from a SPRT-like policy. However, our SPRT model was only a mediocre explanation of people's response times---less so than even the number of times the ball bounced. However, people did not see the number of times that the ball bounced; thus, the effect of the number of bounces is a fairly clear sign that they \emph{are} using simulations in some manner. Rather than a failing of the SPRT model, this may be a failure of our simulation-based model to appropriately account for additional simulation time from bounces.

% However, based on our earlier result that unknown simulation time has little effect, even if we added extra simulation time per bounce to the model, it should not change the best-fitting threshold. Future work will investigate this claim further.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{references}

\end{document}
