\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{color}

\newcommand{\TODO}[1]{\textcolor{red}{[TODO: #1]}}

\title{Think again?\\ Optimal mental simulation tracks problem difficulty}

\author{{\large \bf Jessica B.~Hamrick (jhamrick@berkeley.edu)} \\
  Department of Psychology \\
  University of California, Berkeley
  \AND {\large \bf Kevin A.~Smith (k2smith@ucsd.edu)} \\
  Department of Psychology \\
  University of California, San Diego
  \AND {\large \bf Thomas L.~Griffiths (tom\_griffiths@berkeley.edu)} \\
  Department of Psychology \\
  University of California, Berkeley
  \AND {\large \bf Edward Vul (evul@ucsd.edu)} \\
  Department of Psychology \\
  University of California, San Diego}

\begin{document}

\maketitle

\begin{abstract}
\TODO{}

\textbf{Keywords:} 
add your choice of indexing terms or keywords; kindly use a
semicolon; between each term
\end{abstract}

\section{Introduction}

%% The question we're interested in: how many simulations do people take?
Imagine you are playing a video game---for example, Angry Birds or Desert Golf---in which you need to predict where an object will end up. How do you know where to aim, and how long do you spend thinking about each shot before you take it? Recent research in the domain of intuitive physics and mental simulation has revealed that people make predictions like these by running noisy physical simulations \cite{Smith2012,Battaglia2013,Smith:2013ug,Smith:2013th,Smith:2014tx,Ullman:2014ut,Hamrick:2015}. However, while this research has investigated the \emph{mechanism} by which people make these types of predictions, there has been very little investigation into \emph{how} people actually use this mechanism. If people are running mental simulations, \emph{how many} simulations do they run?

%% What's already been done
According to previous research by \citeA{Vul:2014ba}, a sample-based agent should only take a small number of samples before making a judgment---perhaps even just one. While taking a single sample is clearly suboptimal for each individual judgment, over the long run, taking a small number of samples maximizes expected utility across a large number of judgments. Does this story also hold true for mental simulations? There is some evidence that points to ``yes'': \citeA{Battaglia2013} analyzed the variability of people's responses in tasks concerning towers of building blocks, and found that participants seemed to use between three and seven samples per judgment.

A second question concerns the actual strategy used to take samples. Do people always take exactly the same number of samples for each decision, or do they vary the number of samples based on problem difficulty? While \citeA{Vul:2014ba} looked at a variety of strategies, they concluded that regardless of strategy, the optimal solution was still roughly the same: the agent should take very few samples. However, that does not make the choice of strategy irrelevant. If the strategy tells the agent to vary the number of samples, then we should see variability in the agent's response times. Indeed, such variability can be observed in other types of decision-making tasks, and has been well-modeled by the \textit{sequential probability ratio test} \cite{wald1947sequential}, or SPRT \cite<e.g.>{Gold:2007fo,Ratcliff:2008ux,Bitzer:2014ea}. In SPRT, the agent takes samples until it reaches a threshold number of samples in support of one hypothesis over another hypothesis.

%% What we're going to do and how we're going to do it
In this paper, we investigate the number of simulations that people run when reasoning about a task that requires mental simulation of physics. Specifically, we ask the question: do people vary the number of mental simulations that they run depending on the difficulty of the problem? And, if so, do they do so in a manner consistent with SPRT? To answer these questions, we ran a behavioral experiment in which people watched a ball bouncing around in a box, and had to judge whether it would first go through a hole in the wall, or bounce off that wall. We varied the difficulty of each trial by changing the size of the holes and the margin by which the ball either went through the hole or missed the hole. We hypothesized that people should be faster to respond on ``easy'' trials (e.g., where the ball goes directly through the center of a large hole, or hits the wall very far away from the hole), and slower to respond on ``hard'' trials (e.g., where the ball just barely goes through the hole, or just barely misses the hole). In brief, find that people's responses are well-predicted by a model of simulation, and that their response times are well-predicted by combining that model with a SPRT strategy.

%% What we're going to do and how we're going to do it
The plan of the paper is as follows. First, we formalize our hypothesis for how people respond to the question, ``will the ball go through the hole?'', specifying both a simulation-based model as well as the SPRT strategy. Second, we describe two behavioral experiments, one in which we asked people to respond to the question of ``will the ball go through the hole?'', and a related experiment that we used to determine the parameters for the simulation-based model. Third, we describe the results of the experiments, and compare people's judgments and response times to those from the model. Finally, we discuss the implications of our results on the broader, underlying question: how do people use mental simulation?

\section{Modeling mental simulation with SPRT}

% SPRT
\citeA{Vul:2014ba} examined not just the case where an agent can take a fixed number of samples, but also several policies for taking a variable number of samples depending on how useful the samples are. For example, consider the case where an agent must decide between two options. If they draw samples favoring one of the options with probability 0.95, then the agent can be fairly confident about which is the better option after only a few samples. In contrast, if the probability is 0.55, then the agent would need many samples to be confident about which option is better. Of course, the agent doesn't know what the probability is: that is the point of sampling. One strategy which addresses this issue is known as the \emph{sequential probability ratio test} \cite{wald1947sequential}, or SPRT, and has been used extensively to model a variety of decision making tasks in humans \cite<e.g.>{Gold:2007fo,Ratcliff:2008ux,Bitzer:2014ea}. An agent using SPRT accumulates evidence for one option versus another, and stops sampling once a particular threshold of evidence has been reached. So, rather than optimizing for a fixed number of samples, the agent should optimize for the threshold.

\TODO{probably don't actually need to go through all the math here, can just cite the appendix of one and done?}

% briefly go over one and done math
Here, we consider the binary (or two-alternative forced choice) case, where an agent must choose one of two hypotheses, $H_0$ or $H_1$. The agent may take samples $X_i$ from a Bernoulli distribution parameterized by an unknown parameter $p$, and from these samples estimate $\hat{p}=\frac{1}{N}\sum_{i=1}^N X_i$, where $N$ is the total number of samples. Then, the decision rule which minimizes the probability of error is $\hat{H}(X_1,\ldots{},X_N)=H_0$ when $\hat{p}<0.5$ and $\hat{H}(X_1,\ldots{},X_N)=H_1$ when $\hat{p}>0.5$.

In the best possible case, the agent takes infinite samples and chooses the maximum \emph{a posteriori} (MAP) hypothesis with probability $p$. In practice, the agent cannot take infinite samples. Thus, to determine when to stop sampling (i.e., what the value of $N$ is), we use the \emph{sequential probability ratio test} \cite{wald1947sequential}. When using a SPRT strategy, the agent accumulates $Y_N=\sum_{i=1}^N 2X_i-1$, and stops either when $Y_N=T$ or $Y_N=-T$, for some threshold number of samples $T$. Independent of the number of samples actually taken, the probability of choosing $H_1$ given that $H_1$ is the MAP hypothesis is:
\begin{equation}
\Pr[\hat{H}(Y_N)=H_1\,|\,H_1]=\frac{p^T}{p^T+(1-p)^T},
\label{eq:pr-choose-h1}
\end{equation}
and so the marginal probability of correctly choosing the MAP hypothesis is:
\begin{equation}
\Pr[\mathrm{correct}]=\frac{p^{T+1}+(1-p)^{T+1}}{p^T+(1-p)^T}.
\label{eq:pr-correct}
\end{equation}
Then, the expected utility for a decision, given a threshold and the true probability, is:
\begin{equation}
\mathbb{E}[U\,|\,T,p]=\Pr[\mathrm{correct}]\cdot{}u^{(+)}+(1-\Pr[\mathrm{correct}])\cdot{}u^{(-)},
\label{eq:expected-utility}
\end{equation}
where $u^{(+)}$ is the utility for a correct answer and $u^{(-)}$ is the utility for an incorrect answer.

We are interested not just in how much utility is gained from a single decision, but how much utility is gained from \emph{each additional sample}. Under the SPRT strategy, the probability of taking $N$ samples and then choosing $H_1$ is:
\begin{equation}
\Pr[N,H_1\,|\,T,p]=\left(\frac{2^N}{2T}\right)p^{\frac{N+T}{2}}(1-p)^{\frac{N-T}{2}}\sum_{\nu=1}^{2T-1}\cos\left(\frac{\nu\pi}{2T}\right)^{N-1}\sin\left(\frac{\nu\pi}{2T}\right)\sin\left(\frac{\nu\pi}{2}\right)
\end{equation}
for $N\geq T$ and where $N$ is of the same parity as $T$ \cite[ch.~XIV, eq. 5.7]{Feller:1968ut}. Because we are dealing with binary hypotheses, the marginal probability of $N$ samples is then:
\begin{equation}
\Pr[N\,|\,T,p]=\Pr[N,H_1\,|\,T,p]+\Pr[N,H_1\,|\,T,1-p].
\label{eq:pr-n}
\end{equation}

Putting together Equations \ref{eq:expected-utility} and \ref{eq:pr-n}, the expected rate or return, or utility over time, is:
\begin{equation}
\mathbb{E}[U/t\,|\,T,t_a,t_s]=\int_0^1 \sum_{N=T}^\infty \frac{\mathbb{E}[U\,|\,T,p]\cdot{}t_a}{Nt_s+t_a}\Pr[N\,|\,T,p]\Pr[p]\ \mathrm{d}p,
\label{eq:ror-fixed}
\end{equation}
where $t_s$ is the time it takes to draw a single sample, and $t_a$ is the time it takes to make a decision or execute an action \cite{Vul:2014ba}. Finally, given this equation, we can compute the optimal threshold for an agent using the SPRT strategy:
\begin{equation}
T^*=\mathrm{arg}\max_T \mathbb{E}[U/t\,|\,T,t_a,t_s].
\label{eq:optimal-threshold}
\end{equation}

\section{Modeling physical predictions}

% describe model from sources of uncertainty
As discussed in the introduction, recent research suggests that people can reason about physical scenarios by running noisy physics simulations \cite<e.g.>{Smith2012,Battaglia2013,Smith:2013ug,Smith:2013th,Smith:2014tx,Ullman:2014ut,Hamrick:2015}. Here, we assume this to be the case, and use the model from \citeA{Smith2012}. Briefly, their model incorporates four sources of uncertainty into the physical simulations: perceptual uncertainty over the position of the ball ($\sigma_p$), uncertainty in the initial direction of velocity ($\kappa_v$), ongoing uncertainty in the direction of movement ($\kappa_m$), and uncertainty in the bounce angle ($\kappa_b$). We fit these model parameters to human judgments in a related but separate task. Then, to determine the probability that the ball goes in the hole, we simulated $M=10000$ samples from the model. From these samples, we estimated a truncated normal distribution along the wall with the hole in it, and then computed the probability density lying within the region of the hole.

\TODO{talk about standard deviation adjustment}

\TODO{talk about adjusting for the number of bounces -- or should this be in the results?}

\section{Experiment 1}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/experiment.png}
        \caption{\textbf{Example experimental trial.} Each panel shows a different part of the trial. The left panel shows the initial screen presented to the participant. The middle panel shows the occluded ball, after observing the stimulus presentation. The faded gray line shows the path the ball took during the initial presentation. The right panel shows the final position of the ball, after observing the feedback. As in the middle panel, the faded gray line shows the path of the ball.}
        \label{fig:experiment}
    \end{center}
\end{figure*}

\subsection{Participants}

We recruited $N=328$ participants on Amazon's Mechanical Turk using the psiTurk \cite{McDonnell12} experimental framework. Participants were treated in accordance with UC Berkeley IRB standards and were paid \$0.60 for approximately 6.5 minutes of work. Participants were randomly assigned to one of eight conditions, which determined which stimuli they judged based on a latin-square design (see Stimuli). Additionally, we excluded $N=8$ participants from analysis for answering incorrectly on more than one control trial (see Stimuli), leaving a total of $N=320$ participants.

\subsection{Stimuli}

The stimuli consisted of animations depicting a blue ball with a radius of 10px bouncing around in a box with dimensions 900px $\times$ 650px. All stimuli consisted of two separate animations. The first was the stimulus presentation, which had a duration of 0.775 seconds and depicted the ball moving in a particular direction. The second animation was the feedback, which picked up immediately where the first animation left off, and which had a duration of 1.5 seconds and depicted the ball either going into the hole or bouncing off the wall that contained the hole.\footnote{To enforce the constraint that the ball always travel the same distance during the feedback animation, the $x$-coordinate of the wall with the hole in it was allowed to vary across stimuli.} During the feedback animation, the ball could bounce on the other walls either 0, 1, or 2 times before going into the hole hitting the wall with the hole in it. In both animations, the ball had a velocity of 400px/s. Additionally, in the animations, the path that the ball had traveled so far was drawn with a faded gray line (see Figure \ref{fig:experiment}).

There were 48 different initial animations, and for each of these intial animations, there were four different types of feedback and two different hole sizes, giving a total of eight versions of each stimulus. The four feedback types were: ``far in'' (FI), where the ball went directly through the center of the hole; ``far miss'' (FM), where the ball missed the hole by a wide margin; ``close in'' (CI), where the ball just barely went through the hole; and `` close miss'' (CM), where the ball just barely missed the hole. The two hole sizes were 100px and 200px.

In order to ensure that participants never saw the same initial animation twice, we used a latin-square design of Initial Animation $\times$ Hole Type $\times$ Hole Size. Thus, each participant saw each initial animation exactly once, each hole type exactly 12 times, and each hole size exactly 24 times.

In addition to the 48 experimental trials, there were seven instruction trials and eight control trials, which were the same for all participants. The control trials were designed to be extremely easy and were either of type ``straight hit'' (with a hole size of either 300px or 350px) or ``far miss'' (with a hole size of 100px). Thus, participants saw a total of 63 trials throughout the experiment.

\subsection{Procedure}

The experiment was divided into two phases: the training phase, and the experimental phase. During the training phase, participants made judgments on the seven instruction trials in order to get used to the task. During the experimental phase, participants made judgements on the 48 experimental trials, presented in a random order, as well as the eight control trials, which were also shown in a random order, but interspersed with the experimental trials such that every 8th trial was a control trial.

On each trial, participants were shown the scene, including the initial position of the ball and the location of the hole. Participants were instructed to press the ``space'' key to begin the trial. Immediately upon pressing ``space'', the initial stimulus presentation began. As soon as the initial stimulus animation concluded, a gray box was drawn over the screen, occluding the ball (but not the line depicting the path it had traveled so far; this was left in as a reminder to participants of where the ball had come from). Participants were asked, ``will the ball go in the hole?'', and were instructed to press `q' to respond in the affirmative, and `p' to respond in the negative. Immediately after responding, text appeared saying ``Correct!'' or ``Incorrect.'', depending on the participants' response. Additionally, the gray occluder was removed, and participants were shown the feedback animation. After the feedback animation was complete, the final frame of the animation remained on the screen until participants pressed ``space'' to advance to the next trial. 

Throughout the experiment, participants saw two counters: one showing their total progress (the number of trials completed / total trials), and their score (the number of times they answered correctly / the number of trials completed).

\subsection{Results}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/human-pct-vs-rt.pdf}
        \caption{\textbf{Response times as a function of response.} Left subplot: each bar shows the proportion of participants saying that the ball will go in the hole for a particular feedback type ($x$-axis) and hole size (color). Middle subplot: like the left subplot, but the $y$-axis shows bootstrapped logarithmic means of response times. Right subplot: each point corresponds to a different stimulus, feedback type, and hole size.  The $x$-axis is the proportion of participants saying the ball will go in the hole, and the $y$ axis is the logarithmic mean response time.}
        \label{fig:pct-vs-rt}
    \end{center}
\end{figure*}

\subsubsection{Responses}

On average, participants were correct 76.1\% of the time ($N=17686$) and responded that the ball would go in the hole 52.8\% of the time ($N=17686$). There was a significant effect of feedback type on participants' responses ($\chi^2(3)=4477.2, p<0.001$) as well as a significant effect of hole size ($\chi^2(1)=168.6, p<0.001$). Additionally, there was an interaction between feedback type and hole size ($\chi^2(3)=64.469, p<0.001$). Figure \ref{fig:pct-vs-rt} (left subplot) shows responses as a function of feedback type and hole size.

\subsubsection{Response times}

For all analyses of response time, we computed averages using bootstrapped logarithmic means (exponential of the mean of the log response times), using 1000 bootstrap samples. Participants had an average response time of $M=1310.9$ milliseconds ($SD=1296.6$ms). There were significant effects of both feedback type ($\chi^2(3)=63.611, p<0.001$) and hole size ($\chi^2(1)=8.9808, p<0.01$) on response time, as well as an interaction between feedback type and hole size ($\chi^2(3)=27.146, p<0.001$). However, the hole size was only significant in the case of the ``far in'' (FI) trials ($p<0.001$), in which the ball went straight through the center of the hole. Figure \ref{fig:pct-vs-rt} (middle subplot) shows the response times as a function of feedback type and hole size.

\subsubsection{Relationship of responses to reaction time}

As shown in Figure \ref{fig:pct-vs-rt} (right subplot), we found a clear speed-accuracy tradeoff in participants responses and response times. In particular, participants were on average faster to respond on trials that they were more certain about, and slower to respond to trials that they were unsure about. This tradeoff mirrors the same tradeoff found in SPRT: decisions for which $p\approx0.5$ take much longer to make, because it takes longer to get to one threshold versus the other.

\section{Experiment 2}

Based on the results of Experiment 1, it is clear that people are spending longer on decisions that they are less confident about. In order to determine whether this could be explained by running mental simulations according to the SPRT strategy, we first needed to better characterize the parameters of people's mental simulations. Experiment 2 was designed as a conceptual replication of \citeA{Smith2012}, which would allow us to affirm that people are using mental simulation on these types of tasks, and if so, what the parameters of those simulations are.

\subsection{Participants}

We recruited $N=60$ participants on Amazon's Mechanical Turk using the psiTurk \cite{McDonnell12} experimental framework. Participants were treated in accordance with UC Berkeley IRB standards and were paid \$0.60 for approximately 5 minutes of work. Additionally, we excluded $N=18$ participants from analysis for failing to catch the ball on more than one control trial, leaving a total of $N=42$ participants.

\subsection{Stimuli}

The stimuli were modified versions of those used Experiment 1, with two major differences. First, instead of a wall with a hole in it, there was a paddle of length \TODO{}px that could move up and down the $y$-axis. Second, there was no feedback animation; instead, there was just a single feedback image corresponding to the last frame of the feedback animation. Because there was no feedback animation, there were only 48 stimuli (corresponding to the stimulus presentation animations from Experiment 1), plus the seven instruction trials and eight catch trials.

\subsection{Procedure}

Like Experiment 1, the experiment was divided into two phases: the training phase (consisting of seven trials), and the experimental phase (consisting of the 48 experimental trials, with the eight control trials evenly interspersed). On each trial, participants were shown the scene, including the initial position of the ball and the location of the hole. The paddle begin at the center of the $y$-axis, and was freely moveable along the $y$-axis from the very beginning of the trial. Participants were instructed to press the ``space'' key to begin the trial. 

As in Experiment 1, immediately upon pressing ``space'', the initial stimulus presentation began, after which an occluder appeared. However, unlike Experiment 1, as soon as the occluder appeared, a timer also appeared and begin counting down for \TODO{} seconds. During this time, participants had to move the paddle to try to catch the ball in the position it would be when the timer was up.When the timer countdown finished, the paddle froze at its current location, and the occluder was removed and the full path of the ball revealed. If the ball was on the paddle, then participants were told, ``You caught the ball!''. If the ball was not on the paddle, participants were told ``Oops, you didn't catch the ball.'' Participants were then instructed to press ``space'' to continue, at which point the next trial would begin.

Throughout the experiment, participants saw two counters: one showing their total progress (the number of trials completed / total trials), and their score (the number of times they caught the ball / the number of trials completed).

\subsection{Results}

\TODO{summarize results showing that experimen 2 replicates Smith \& Vul (2014), and talk about fitting the model parameters}

\section{Does SPRT explain human judgments?}

We fit participant responses and response times to the SPRT model with different values of $T$, and found that $T=2$ to have the best fit. Participant responses were well correlated with the SPRT model ($r=0.81$, 95\% CI [0.77, 0.84]), and slightly less so with a SPRT model with $T=2$ ($r=0.78$, 95\% CI [0.74, 0.81]). There was also a significant correlation with resonse time for the $T=2$ SPRT model ($r=0.35$, 95\% CI [0.27, 0.43]). However, it turns out that the number of bounces is a much better predictor of people's response times than the model itself. 

\TODO{explain SPRT results}



\section{Discussion}

\TODO{}

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{references}

\end{document}
